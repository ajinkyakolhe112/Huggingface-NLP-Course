{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598050713539124},\n",
       " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "classifier = transformers.pipeline(\"sentiment-analysis\")\n",
    "classifier(\n",
    "    [\n",
    "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "        \"I hate this so much!\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behind the Pipeline API\n",
    "\n",
    "4 Stage Pipeline\n",
    "1. dataset\n",
    "2. language tokenizer\n",
    "3. pretrained_model\n",
    "4. model output\n",
    "![](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. initialize tokenizer & model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stages of learning a new Language\n",
    "1. Learn vocabulary\n",
    "2. Learn meaning of words\n",
    "3. Learn syntax & grammar rules\n",
    "4. Speak the language\n",
    "\n",
    "Similarly, In Artificial NLP\n",
    "1. Build vocabulary with `tokenizer` & map it to numbers\n",
    "2. Replace numbers with vectors with semantic meaning via embedding_value via `model`\n",
    "3. Learn syntax & grammar rules via `model`\n",
    "4. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "demo_sentence                   = \"three words sentence\"    # One sentence containing 3 Words\n",
    "word_to_number_mapping          = [4, 6, 3 ]                # 3 Words, Each number mapped to a Word\n",
    "one_number_to_meaning_mapping   = torch.zeros(512)          # Meaning in 512 int vector\n",
    "demo_sentence_with_meaning      = torch.zeros(3, 512)       # 3 Words, Each word 512 int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch vs Tensorflow code\n",
    "\n",
    "```python\n",
    "import transformers\n",
    "\n",
    "checkpoint  = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "tokenizer   = transformers. AutoTokenizer.                        from_pretrained(checkpoint)\n",
    "model       = transformers. TFAutoModel.                          from_pretrained(checkpoint)\n",
    "model       = transformers. TFAutoModelForSequenceClassification. from_pretrained(checkpoint)\n",
    "\n",
    "tokenizer   = transformers. AutoTokenizer.                        from_pretrained(checkpoint)\n",
    "model       = transformers. AutoModel.                            from_pretrained(checkpoint)\n",
    "model       = transformers. AutoModelForSequenceClassification.   from_pretrained(checkpoint)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "checkpoint          = \"bert-base-cased\"\n",
    "\n",
    "tokenizer           = transformers. AutoTokenizer.                        from_pretrained(checkpoint)\n",
    "\n",
    "model_auto          = transformers. AutoModel.                            from_pretrained(checkpoint)\n",
    "model_auto_sequence = transformers. AutoModelForSequenceClassification.   from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. tokenizer & input to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,   146,   112,  1396,  1151,  2613,  1111,   170, 20164, 10932,\n",
       "          2271,  7954,  1736,  1139,  2006,  1297,   119,   102],\n",
       "        [  101,   146,  4819,  1142,  1177,  1277,   106,   102,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "\n",
    "numeric_ids = tokenizer(raw_inputs , padding=True , return_tensors=\"pt\")    # Numeric ids => as PYTORCH TENSORS\n",
    "\n",
    "numeric_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer returns TWO things => dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "Row Number => 1, \n",
      "\t input_ids => tensor([  101,   146,   112,  1396,  1151,  2613,  1111,   170, 20164, 10932,\n",
      "         2271,  7954,  1736,  1139,  2006,  1297,   119,   102]), \n",
      "\t attention_mask => tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Row Number => 2, \n",
      "\t input_ids => tensor([ 101,  146, 4819, 1142, 1177, 1277,  106,  102,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0]), \n",
      "\t attention_mask => tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "print(f'tokenizer returns TWO things => {numeric_ids.keys()}')\n",
    "\n",
    "index = 0\n",
    "while index < len(numeric_ids['input_ids']):\n",
    "    print(f\"Row Number => {index+1}, \\n\\t input_ids => { numeric_ids['input_ids'][index]}, \\n\\t attention_mask => {numeric_ids['attention_mask'][index]}\")\n",
    "    index = index + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. model output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.4222,  0.4443, -0.0659,  ..., -0.1958,  0.3611,  0.1284],\n",
       "         [ 0.5728, -0.1593,  0.6014,  ..., -0.1134,  0.1791,  0.1787],\n",
       "         [ 0.4699,  0.4214,  0.1695,  ...,  0.2386,  0.9851, -0.1236],\n",
       "         ...,\n",
       "         [ 0.5847,  0.2552,  0.0266,  ...,  0.7203,  0.0650,  0.4277],\n",
       "         [ 0.5573,  0.4506,  0.0353,  ..., -0.0607,  0.4209, -0.2525],\n",
       "         [ 0.7136,  1.2932, -0.2937,  ...,  0.2917,  0.4270, -0.3874]],\n",
       "\n",
       "        [[ 0.4829,  0.4291,  0.0264,  ..., -0.1489,  0.2953, -0.3113],\n",
       "         [ 0.2735,  0.4520,  0.2760,  ...,  0.2572,  0.2059,  0.4097],\n",
       "         [ 0.1391,  0.4234, -0.3385,  ...,  0.5858, -0.0834,  0.4344],\n",
       "         ...,\n",
       "         [ 0.0709,  0.4650, -0.1060,  ...,  0.2954,  0.1990,  0.1774],\n",
       "         [ 0.1649,  0.4855, -0.0801,  ...,  0.3485,  0.1970,  0.1701],\n",
       "         [ 0.2887,  0.4945,  0.0196,  ...,  0.2895,  0.2056,  0.0399]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.6911,  0.4541,  0.9999,  ...,  1.0000, -0.8468,  0.9912],\n",
       "        [-0.5548,  0.4425,  0.9998,  ...,  1.0000, -0.9216,  0.9935]],\n",
       "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_auto = model_auto     (**numeric_ids)\n",
    "\n",
    "outputs_auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 18, 768]) torch.Size([2, 768])\n"
     ]
    }
   ],
   "source": [
    "all_extracted_features_map  = outputs_auto['last_hidden_state']\n",
    "compressed_features         = outputs_auto['pooler_output']\n",
    "\n",
    "print(all_extracted_features_map.shape, compressed_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL_AUTO OUTPUT FORMAT: dict_keys(['last_hidden_state', 'pooler_output', 'hidden_states', 'past_key_values', 'attentions', 'cross_attentions']) \n"
     ]
    }
   ],
   "source": [
    "print(f'MODEL_AUTO OUTPUT FORMAT: {vars(outputs_auto).keys()} ' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 18, 768])\n",
      "Number of Senteces = 2, Num of Words in Each Sentece = 18, Every Word Embedding Length = 768\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE, SENTENCE_LENGTH, WORD_EMBEDDING_SIZE = outputs_auto['last_hidden_state'].shape\n",
    "print(outputs_auto['last_hidden_state'].shape)\n",
    "print(f'Number of Senteces = {BATCH_SIZE}, Num of Words in Each Sentece = {SENTENCE_LENGTH}, Every Word Embedding Length = {WORD_EMBEDDING_SIZE}')\n",
    "\n",
    "# EMBEDDING SIZE or HIDDEN SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL_AUTO_SEQUENCE OUTPUT FORMAT: {'loss': None, 'logits': tensor([[-1.5607,  1.6123],\n",
      "        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward0>), 'hidden_states': None, 'attentions': None} \n"
     ]
    }
   ],
   "source": [
    "outputs_auto_sequence = model_auto_sequence     (**numeric_ids)\n",
    "print(f'MODEL_AUTO_SEQUENCE OUTPUT FORMAT: {vars(outputs_auto_sequence)} ' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1532, 0.4257],\n",
      "        [0.2897, 0.3980]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4323, 0.5677],\n",
      "        [0.4730, 0.5270]], grad_fn=<SoftmaxBackward0>)\n",
      "{0: 'LABEL_0', 1: 'LABEL_1'}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"bert-base-cased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**numeric_ids)\n",
    "\n",
    "print(outputs.logits)\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(predictions)\n",
    "\n",
    "print(model.config.id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertModel(\n",
       "  (embeddings): Embeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_auto_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terms\n",
    "\n",
    "1. Head\n",
    "2. Hidden State\n",
    "3. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
