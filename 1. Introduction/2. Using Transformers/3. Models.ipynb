{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initializing model & tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "config              = transformers.BertConfig()\n",
    "model_untrained     = transformers.BertModel(config)\n",
    "\n",
    "model_pretrained    = transformers.BertModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.BertTokenizerFast.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35mdatasets\u001b[m\u001b[m \u001b[34mhub\u001b[m\u001b[m      \u001b[34mmodules\u001b[m\u001b[m  token\n",
      "\u001b[35m/Users/ajinkya/.cache/huggingface/datasets\u001b[m\u001b[m\n",
      "/Users/ajinkya/.cache/huggingface/token\n",
      "\n",
      "/Users/ajinkya/.cache/huggingface/hub:\n",
      "\u001b[34mmodels--1vash--mnist_demo_model\u001b[m\u001b[m\n",
      "\u001b[34mmodels--ajinkyakolhe112--mnist-test\u001b[m\u001b[m\n",
      "\u001b[34mmodels--bert-base-cased\u001b[m\u001b[m\n",
      "\u001b[34mmodels--distilbert-base-uncased-finetuned-sst-2-english\u001b[m\u001b[m\n",
      "\u001b[34mmodels--facebook--bart-large-mnli\u001b[m\u001b[m\n",
      "\u001b[34mmodels--google-bert--bert-base-uncased\u001b[m\u001b[m\n",
      "\u001b[34mmodels--gpt2\u001b[m\u001b[m\n",
      "\u001b[34mmodels--jonesmarquelle--classify-nums\u001b[m\u001b[m\n",
      "\u001b[34mmodels--timm--resnest26d.gluon_in1k\u001b[m\u001b[m\n",
      "\u001b[34mmodels--timm--resnet50.a1_in1k\u001b[m\u001b[m\n",
      "\u001b[34mmodels--timm--xception41.tf_in1k\u001b[m\u001b[m\n",
      "version.txt\n",
      "\n",
      "/Users/ajinkya/.cache/huggingface/modules:\n",
      "__init__.py      \u001b[34mdatasets_modules\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "! ls  ~/.cache/huggingface/ \n",
    "! ls  ~/.cache/huggingface/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdirectory_on_my_computer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(\"directory_on_my_computer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: directory_on_my_computer: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "! ls directory_on_my_computer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. tokenizer on input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL INPUT FORMAT: {'input_ids': tensor([[  101,   146,   112,  1396,  1151,  2613,  1111,   170, 20164, 10932,\n",
      "          2271,  7954,  1736,  1139,  2006,  1297,   119,   102],\n",
      "        [  101,   146,  4819,  1142,  1177,  1277,   106,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "MODEL OUTPUT FORMAT: {'last_hidden_state': tensor([[[ 0.4222,  0.4443, -0.0659,  ..., -0.1958,  0.3611,  0.1284],\n",
      "         [ 0.5728, -0.1593,  0.6014,  ..., -0.1134,  0.1791,  0.1787],\n",
      "         [ 0.4699,  0.4214,  0.1695,  ...,  0.2386,  0.9851, -0.1236],\n",
      "         ...,\n",
      "         [ 0.5847,  0.2552,  0.0266,  ...,  0.7203,  0.0650,  0.4277],\n",
      "         [ 0.5573,  0.4506,  0.0353,  ..., -0.0607,  0.4209, -0.2525],\n",
      "         [ 0.7136,  1.2932, -0.2937,  ...,  0.2917,  0.4270, -0.3874]],\n",
      "\n",
      "        [[ 0.4829,  0.4291,  0.0264,  ..., -0.1489,  0.2953, -0.3113],\n",
      "         [ 0.2735,  0.4520,  0.2760,  ...,  0.2572,  0.2059,  0.4097],\n",
      "         [ 0.1391,  0.4234, -0.3385,  ...,  0.5858, -0.0834,  0.4344],\n",
      "         ...,\n",
      "         [ 0.0709,  0.4650, -0.1060,  ...,  0.2954,  0.1990,  0.1774],\n",
      "         [ 0.1649,  0.4855, -0.0801,  ...,  0.3485,  0.1970,  0.1701],\n",
      "         [ 0.2887,  0.4945,  0.0196,  ...,  0.2895,  0.2056,  0.0399]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), 'pooler_output': tensor([[-0.6911,  0.4541,  0.9999,  ...,  1.0000, -0.8468,  0.9912],\n",
      "        [-0.5548,  0.4425,  0.9998,  ...,  1.0000, -0.9216,  0.9935]],\n",
      "       grad_fn=<TanhBackward0>), 'hidden_states': None, 'past_key_values': None, 'attentions': None, 'cross_attentions': None} \n"
     ]
    }
   ],
   "source": [
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "\n",
    "PYTORCH_TENSOR_FORMAT = \"pt\"\n",
    "\n",
    "inputs = tokenizer(raw_inputs   , return_tensors= PYTORCH_TENSOR_FORMAT, padding=True, truncation= False )\n",
    "print(f'MODEL INPUT FORMAT: {inputs}')\n",
    "\n",
    "outputs = model_pretrained(**inputs)\n",
    "print(f'MODEL OUTPUT FORMAT: {vars(outputs)} ' )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===========================================================================\n",
       "Layer (type:depth-idx)                             Param #\n",
       "===========================================================================\n",
       "BertModel                                          --\n",
       "├─BertEmbeddings: 1-1                              --\n",
       "│    └─Embedding: 2-1                              22,268,928\n",
       "│    └─Embedding: 2-2                              393,216\n",
       "│    └─Embedding: 2-3                              1,536\n",
       "│    └─LayerNorm: 2-4                              1,536\n",
       "│    └─Dropout: 2-5                                --\n",
       "├─BertEncoder: 1-2                                 --\n",
       "│    └─ModuleList: 2-6                             --\n",
       "│    │    └─BertLayer: 3-1                         7,087,872\n",
       "│    │    └─BertLayer: 3-2                         7,087,872\n",
       "│    │    └─BertLayer: 3-3                         7,087,872\n",
       "│    │    └─BertLayer: 3-4                         7,087,872\n",
       "│    │    └─BertLayer: 3-5                         7,087,872\n",
       "│    │    └─BertLayer: 3-6                         7,087,872\n",
       "│    │    └─BertLayer: 3-7                         7,087,872\n",
       "│    │    └─BertLayer: 3-8                         7,087,872\n",
       "│    │    └─BertLayer: 3-9                         7,087,872\n",
       "│    │    └─BertLayer: 3-10                        7,087,872\n",
       "│    │    └─BertLayer: 3-11                        7,087,872\n",
       "│    │    └─BertLayer: 3-12                        7,087,872\n",
       "├─BertPooler: 1-3                                  --\n",
       "│    └─Linear: 2-7                                 590,592\n",
       "│    └─Tanh: 2-8                                   --\n",
       "===========================================================================\n",
       "Total params: 108,310,272\n",
       "Trainable params: 108,310,272\n",
       "Non-trainable params: 0\n",
       "==========================================================================="
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model_pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' vocab_size, n_heads, n_layers, label2id, id2label, hidden_dim, dim '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pretrained.config\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- vocab_size\n",
    "- n_heads\n",
    "- n_layers\n",
    "- label2id\n",
    "- id2label\n",
    "- hidden_dim\n",
    "- dim "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
